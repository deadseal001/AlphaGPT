# model_core/engine.py (å»ºè®®å…¨æ–‡ä»¶æ›¿æ¢)

import torch
from torch.distributions import Categorical
from tqdm import tqdm
import json
import numpy as np # éœ€è¦ import numpy

from .config import ModelConfig
from .data_loader import CryptoDataLoader
from .alphagpt import AlphaGPT, NewtonSchulzLowRankDecay, StableRankMonitor
from .vm import StackVM
from .backtest import MemeBacktest

class AlphaEngine:
    def __init__(self, use_lord_regularization=True, lord_decay_rate=1e-3, lord_num_iterations=5):
        """
        Initialize AlphaGPT training engine.
        """
        self.loader = CryptoDataLoader()
        self.loader.load_data()
        
        self.model = AlphaGPT().to(ModelConfig.DEVICE)
        
        # Standard optimizer
        self.opt = torch.optim.AdamW(self.model.parameters(), lr=1e-3)
        
        # Low-Rank Decay regularizer
        self.use_lord = use_lord_regularization
        if self.use_lord:
            self.lord_opt = NewtonSchulzLowRankDecay(
                self.model.named_parameters(),
                decay_rate=lord_decay_rate,
                num_iterations=lord_num_iterations,
                target_keywords=["q_proj", "k_proj", "attention", "qk_norm"]
            )
            self.rank_monitor = StableRankMonitor(
                self.model,
                target_keywords=["q_proj", "k_proj"]
            )
        else:
            self.lord_opt = None
            self.rank_monitor = None
        
        self.vm = StackVM()
        self.bt = MemeBacktest()
        
        self.best_score = -float('inf')
        self.best_formula = None
        self.training_history = {
            'step': [],
            'avg_reward': [],
            'best_score': [],
            'stable_rank': []
        }

    def train(self):
        print("ğŸš€ Starting Meme Alpha Mining with LoRD Regularization..." if self.use_lord else "ğŸš€ Starting Meme Alpha Mining...")
        
        pbar = tqdm(range(ModelConfig.TRAIN_STEPS))
        
        for step in pbar:
            bs = ModelConfig.BATCH_SIZE
            inp = torch.zeros((bs, 1), dtype=torch.long, device=ModelConfig.DEVICE)
            
            log_probs = []
            tokens_list = []
            
            # ç”Ÿæˆé˜¶æ®µ
            for _ in range(ModelConfig.MAX_FORMULA_LEN):
                logits, _, _ = self.model(inp)
                dist = Categorical(logits=logits)
                action = dist.sample()
                
                log_probs.append(dist.log_prob(action))
                tokens_list.append(action)
                inp = torch.cat([inp, action.unsqueeze(1)], dim=1)
            
            seqs = torch.stack(tokens_list, dim=1)
            
            rewards = torch.zeros(bs, device=ModelConfig.DEVICE)
            
            # --- ä¼˜åŒ–æ ¸å¿ƒï¼šBatch å†…å»é‡ (Batch Deduplication) ---
            # è½¬æ¢æˆ list of tuples ä»¥ä¾¿å“ˆå¸Œå»é‡
            seqs_list = seqs.tolist()
            seqs_tuples = [tuple(s) for s in seqs_list]
            unique_formulas = set(seqs_tuples)
            
            # ç¼“å­˜å½“å‰ Batch çš„è®¡ç®—ç»“æœ
            formula_rewards_map = {}
            
            # åªå¯¹å”¯ä¸€çš„å…¬å¼è¿›è¡Œå›æµ‹
            for formula_tuple in unique_formulas:
                formula = list(formula_tuple)
                
                # VM æ‰§è¡Œ
                res = self.vm.execute(formula, self.loader.feat_tensor)
                
                if res is None:
                    formula_rewards_map[formula_tuple] = -5.0
                    continue
                
                if res.std() < 1e-4:
                    formula_rewards_map[formula_tuple] = -2.0
                    continue
                
                # å›æµ‹
                score, ret_val = self.bt.evaluate(res, self.loader.raw_data_cache, self.loader.target_ret)
                formula_rewards_map[formula_tuple] = score
                
                # è®°å½•æœ€ä½³
                if score.item() > self.best_score:
                    self.best_score = score.item()
                    self.best_formula = formula
                    tqdm.write(f"[!] New King: Score {score:.2f} | Ret {ret_val:.2%} | Formula {formula}")
            
            # å°†åˆ†æ•°æ˜ å°„å›åŸæ¥çš„ Batch ç´¢å¼•
            for i in range(bs):
                rewards[i] = formula_rewards_map[seqs_tuples[i]]
            # ----------------------------------------------------
            
            # Normalize rewards
            adv = (rewards - rewards.mean()) / (rewards.std() + 1e-5)
            
            loss = 0
            for t in range(len(log_probs)):
                loss += -log_probs[t] * adv
            loss = loss.mean()
            
            if torch.isnan(loss) or torch.isinf(loss):
                continue 
            
            self.opt.zero_grad()
            loss.backward()
            self.opt.step()
            
            if self.use_lord:
                self.lord_opt.step()
            
            # Logging
            avg_reward = rewards.mean().item()
            postfix_dict = {'AvgRew': f"{avg_reward:.3f}", 'BestScore': f"{self.best_score:.3f}"}
            
            if self.use_lord and step % 100 == 0:
                stable_rank = self.rank_monitor.compute()
                postfix_dict['Rank'] = f"{stable_rank:.2f}"
                self.training_history['stable_rank'].append(stable_rank)
            
            self.training_history['step'].append(step)
            self.training_history['avg_reward'].append(avg_reward)
            self.training_history['best_score'].append(self.best_score)
            
            pbar.set_postfix(postfix_dict)

        # Save results
        with open("best_meme_strategy.json", "w") as f:
            json.dump(self.best_formula, f)
        import json as js
        with open("training_history.json", "w") as f:
            js.dump(self.training_history, f)
        
        print(f"\nâœ“ Training completed!")
        print(f"  Best score: {self.best_score:.4f}")

if __name__ == "__main__":
    eng = AlphaEngine(use_lord_regularization=True)
    eng.train()